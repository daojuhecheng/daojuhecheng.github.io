---
title: 【模型解释五】
tags: [模型解释]

author: Yude Bai
date: 2021-12-21
img: /medias/featureimages/13.jpg
summary: 模型可解释性 attention 一

toc: true
---


# :whale: 模型可解释性 attention 一 :whale:

## 1、预训练模型的可解释性
预训练模型包括 pre-train 和 fine-tune 两个阶段，模型在这两个阶段究竟学到了什么？以 BERT 为例，主流的分析方法有两种 (参考 [深度学习预训练模型可解释性概览](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/104470992/))。

一是 **probing tasks**，即设定一些简单的 NLP 任务，将 BERT 的每一层的输出都作为 embedding (neural features of sample)，并将其传至后接的简单的分类器，从而验证使用不同层时的模型的性能。
二是 **visualization**，在 BERT 的多层 Transformer 中存在 multi-head self-attention，其中各个 word 之间通过交互会得到一个 attention score。通过可视化该 score，即可得知不同层或不同 head 中被关注的 word。


## 2、BERT 中的 probing tasks

在 [Open Sesame: Getting Inside BERT’s Linguistic Knowledge](https://arxiv.org/pdf/1906.01698.pdf) (2019 ACL Workshop BlackboxNLP) 一文中，作者把 BERT 的每一层的 embedding 都接上一个简单的分类器做训练 (用于完成不同的分类任务)，每一个 token 会对应一个结果 (如下图所示)，其 label 则为一个 one hot 化的向量，损失函数可以简单设定为交叉熵。

![ACL 1 token](https://img-blog.csdnimg.cn/f50dd07fe35949009e21f36746dc10eb.png#pic_center)

我们主要关注其中关于 layer 的 ablation study，如下图所示。

![ablation study of attention layer](https://img-blog.csdnimg.cn/1e36c79156e84ecbb2d5db7c55beae00.png)

当 layer 递增由 1 变化为 4 时，acc 急剧增加并接近最大值，layer 继续增加，acc 则有所下降。
作者认为浅层的 info 更具象化 (localized)，其所包含的 context 不足以 insure acc。随着 self-attention layer 的增加，info 会逐渐抽象化 (abstract)，以 4~6 layers 为限，acc 最高。随后继续增加 layer，info 则过于抽象，反而会使得 acc 有所下降。


## 3、BERT 中的 visualization

在 [What does BERT look at? An Analysis of BERT’s Attention](https://arxiv.org/pdf/1906.04341.pdf) (2019 ACL Workshop BlackboxNLP) 中，作者通过可视化 BERT 的 attention 层来了解各个 attention 层的关注点。

如下图所示，有的 attention 是 broadly 形式 (关注所有的词)，有的 attention 关注其下一个 token，有的 attention 集中关注 [SEP] 符号，还有的 attention 会关注标点符号。

![ACL 2 attention](https://img-blog.csdnimg.cn/1a6040ef882e4b74948635d765f2a80d.png)

其他更具体的 experimental settings 可参考 [原文](https://arxiv.org/pdf/1906.04341.pdf)。




![](https://img-blog.csdnimg.cn/3bc6d0b482ec453db5c3039a18ec3e9a.png#pic_center)
