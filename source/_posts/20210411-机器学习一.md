---
title: 【机器学习一】
tags: [机器学习]

author: Yude Bai
date: 2021-04-11
img: /medias/featureimages/21.jpg
summary: 集成学习相关代码和博文整理

toc: true
---


# :whale: 集成学习相关代码和博文整理 :whale:


## 1、Tree + boost GitHub
[Github 链接参考](https://github.com/frank0532/decisiontree-randomforest-adaboost-GBDT-xgboost-lightGBM-catBoost)。


## 2、GBM（Gradient Boosting Machine）
[GBM 理解](https://github.com/frank0532/decisiontree-randomforest-adaboost-GBDT-xgboost-lightGBM-catBoost)，该博客中的公式色系和注释非常便于辅助理解。
 - 三种损失函数，square loss，absolute loss，huber loss。
 - AdaBoost 采用 square loss，GBDT 采用后面两种损失函数之一。


## 3、Tree 系列一
主要介绍[ID3、C4.5、CART](https://zhuanlan.zhihu.com/p/85731206)。


## 4、Tree 系列二
主要介绍[Random Forest、Adaboost、GBDT](https://zhuanlan.zhihu.com/p/86263786)。
 - 集成学习三大类，Bagging（RF），Boosting（GBDT） 和 Stacking。
 - 偏差（Bias）描述的是预测值和真实值之差；方差（Variance）描述的是预测值作为随机变量的离散程度。如下图所示。![偏差与方差](https://img-blog.csdnimg.cn/20210411215845631.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ppbW9zYW5ndGlhbg==,size_16,color_FFFFFF,t_70#pic_center)


## 5、Tree 系列三
主要介绍[XGBoost、LightGBM](https://zhuanlan.zhihu.com/p/87885678)。
 - 直方图算法可以显著降低内存消耗。


## 6、LightGBM 安装教程
参考[LightGBM 教程](https://blog.csdn.net/weixin_38569817/article/details/78808535)。
 - 辅助理解 LightGBM 中的直方图算法等。
 - 安装命令，pip install lightgbm。
参考[LightGBM 调参](https://blog.csdn.net/olizxq/article/details/89222908)。
 - sklearn 中 GBDT 和 LightGBM 的参数介绍。


![](https://img-blog.csdnimg.cn/2021041122081610.png#pic_center)

